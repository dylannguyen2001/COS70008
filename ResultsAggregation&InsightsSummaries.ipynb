{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebbcef7b-6f08-413a-a2c0-52f3c5e6409a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote to: C:\\Users\\Dylan\\OneDrive - Swinburne University\\COS70008\\Scripts\\risk_analysis\\results\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "SCRIPTS = Path(r\"C:\\Users\\Dylan\\OneDrive - Swinburne University\\COS70008\\Scripts\")\n",
    "\n",
    "p_emails_clean   = SCRIPTS / \"data_clean\" / \"Emails_clean.parquet\"               \n",
    "p_risk_email     = SCRIPTS / \"risk_analysis\" / \"RiskScores.parquet\"              \n",
    "p_sent_email     = SCRIPTS / \"sentiment_analysis\" / \"SentimentScores.parquet\"      \n",
    "\n",
    "p_sent_emotion   = SCRIPTS / \"nlp_sentiment\" / \"SentimentScores_emotion.parquet\"  \n",
    "p_risk_threads   = SCRIPTS / \"Threads\" / \"RiskScores_threads.parquet\"              \n",
    "p_thread_index   = SCRIPTS / \"Threads\" / \"ThreadIndex.parquet\"                   \n",
    "\n",
    "p_comm_meta      = SCRIPTS / \"network_graphs\" / \"Communities_with_meta.parquet\"    \n",
    "p_comm           = SCRIPTS / \"network_graphs\" / \"Communities.parquet\"             \n",
    "p_nodeindex      = SCRIPTS / \"network_graphs\" / \"NodeIndex.parquet\"                \n",
    "\n",
    "OUT_DIR = SCRIPTS / \"risk_analysis\" / \"results\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def file_ok(p: Path) -> bool:\n",
    "    return p.exists() and p.is_file()\n",
    "\n",
    "def read_px(p: Path):\n",
    "    return pd.read_parquet(p, engine=\"pyarrow\", memory_map=False)\n",
    "\n",
    "def safe_merge(left, right, on, how=\"left\"):\n",
    "    if right is None or len(right) == 0:\n",
    "        return left.copy()\n",
    "    return left.merge(right, on=on, how=how)\n",
    "\n",
    "def normalize_sentiment_rows(df, label_col, score_col, out_value_col=\"sent_value\"):\n",
    "    if df is None or len(df) == 0:\n",
    "        return df\n",
    "    s = df[label_col].astype(\"string\").str.lower()\n",
    "    score = pd.to_numeric(df[score_col], errors=\"coerce\").fillna(0.0)\n",
    "    val = np.where(s.str.contains(\"pos\"), +1.0 * score,\n",
    "          np.where(s.str.contains(\"neg\"), -1.0 * score, 0.0))\n",
    "    df[out_value_col] = val.astype(float)\n",
    "    return df\n",
    "\n",
    "def dominant_label(df, key_cols, label_col, score_col, out_col):\n",
    "    \"\"\"Pick the highest-scoring label per key.\"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return pd.DataFrame(columns=key_cols + [out_col])\n",
    "    idx = (df.sort_values(score_col, ascending=False)\n",
    "             .drop_duplicates(key_cols))\n",
    "    return idx[key_cols + [label_col]].rename(columns={label_col: out_col})\n",
    "\n",
    "\n",
    "assert file_ok(p_emails_clean), f\"Missing {p_emails_clean}\"\n",
    "assert file_ok(p_risk_email),   f\"Missing {p_risk_email}\"\n",
    "assert file_ok(p_sent_email),   f\"Missing {p_sent_email}\"\n",
    "\n",
    "emails_clean = read_px(p_emails_clean)\n",
    "risk_raw     = read_px(p_risk_email)\n",
    "sent_email   = read_px(p_sent_email)\n",
    "\n",
    "sent_emotion = read_px(p_sent_emotion) if file_ok(p_sent_emotion) else None\n",
    "risk_threads = read_px(p_risk_threads) if file_ok(p_risk_threads) else None\n",
    "thread_index = read_px(p_thread_index) if file_ok(p_thread_index) else None\n",
    "comm_meta    = read_px(p_comm_meta) if file_ok(p_comm_meta) else None\n",
    "comm         = read_px(p_comm) if (comm_meta is None and file_ok(p_comm)) else None\n",
    "node_index   = read_px(p_nodeindex) if (comm_meta is None and file_ok(p_nodeindex)) else None\n",
    "\n",
    "\n",
    "need = {\"email_id\",\"person_id\",\"dt_utc\"}\n",
    "missing = need - set(emails_clean.columns)\n",
    "assert not missing, f\"Emails_clean missing columns: {missing}\"\n",
    "\n",
    "emails_clean[\"dt_utc\"] = pd.to_datetime(emails_clean[\"dt_utc\"], utc=True, errors=\"coerce\")\n",
    "emails_clean[\"yyyymm\"] = emails_clean[\"dt_utc\"].dt.strftime(\"%Y%m\")\n",
    "\n",
    "\n",
    "if \"final_score\" not in risk_raw.columns:\n",
    "    raise ValueError(\"RiskScores.parquet must include 'final_score' (overall email risk).\")\n",
    "risk_email = risk_raw[[\"email_id\", \"final_score\"]].copy()\n",
    "risk_email[\"final_score\"] = pd.to_numeric(risk_email[\"final_score\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "if \"risk_label\" in risk_raw.columns:\n",
    "    dom_risk = risk_raw[[\"email_id\", \"risk_label\"]].rename(columns={\"risk_label\": \"top_risk_cat\"})\n",
    "else:\n",
    "    hit_cols = [c for c in risk_raw.columns if c.startswith(\"hits_\")]\n",
    "    if hit_cols:\n",
    "        long_hits = (risk_raw[[\"email_id\"] + hit_cols]\n",
    "                     .melt(id_vars=[\"email_id\"], var_name=\"risk_category\", value_name=\"hits\"))\n",
    "        long_hits[\"risk_category\"] = long_hits[\"risk_category\"].str.replace(r\"^hits_\", \"\", regex=True)\n",
    "        dom_risk = (long_hits.sort_values([\"email_id\",\"hits\"], ascending=[True,False])\n",
    "                            .drop_duplicates([\"email_id\"])\n",
    "                            .rename(columns={\"risk_category\":\"top_risk_cat\"})\n",
    "                            [[\"email_id\",\"top_risk_cat\"]])\n",
    "    else:\n",
    "        dom_risk = pd.DataFrame({\"email_id\": risk_email[\"email_id\"], \"top_risk_cat\": np.nan})\n",
    "\n",
    "sent_email = sent_email.rename(columns={\"label\":\"sentiment_label\",\"score\":\"sentiment_score\"})\n",
    "if \"sentiment_score\" not in sent_email.columns:\n",
    "    cand = [c for c in sent_email.columns if \"score\" in c.lower()]\n",
    "    sent_email[\"sentiment_score\"] = pd.to_numeric(sent_email[cand[0]], errors=\"coerce\").fillna(0.0) if cand else 0.0\n",
    "if \"sentiment_label\" not in sent_email.columns:\n",
    "    cand = [c for c in sent_email.columns if \"label\" in c.lower()]\n",
    "    sent_email[\"sentiment_label\"] = sent_email[cand[0]] if cand else \"neutral\"\n",
    "sent_email = normalize_sentiment_rows(sent_email, \"sentiment_label\", \"sentiment_score\", \"sent_value\")\n",
    "sent_best = (sent_email.sort_values(\"sentiment_score\", ascending=False)\n",
    "             .drop_duplicates([\"email_id\"])\n",
    "             [[\"email_id\",\"sentiment_label\",\"sentiment_score\",\"sent_value\"]])\n",
    "\n",
    "if sent_emotion is not None and len(sent_emotion):\n",
    "    sent_emotion = sent_emotion.rename(columns={\"label\":\"emotion_label\",\"score\":\"emotion_score\"})\n",
    "    dom_emotion = dominant_label(sent_emotion, [\"email_id\"], \"emotion_label\", \"emotion_score\", \"top_emotion\")\n",
    "else:\n",
    "    dom_emotion = None\n",
    "\n",
    "em_base = emails_clean[[\"email_id\",\"person_id\",\"dt_utc\",\"yyyymm\"]].drop_duplicates()\n",
    "em_join = em_base.merge(risk_email, on=\"email_id\", how=\"left\")\n",
    "em_join = safe_merge(em_join, dom_risk,  on=[\"email_id\"])\n",
    "em_join = safe_merge(em_join, sent_best, on=[\"email_id\"])\n",
    "em_join = safe_merge(em_join, dom_emotion, on=[\"email_id\"])\n",
    "\n",
    "for c in [\"final_score\",\"sentiment_score\",\"sent_value\"]:\n",
    "    if c in em_join.columns:\n",
    "        em_join[c] = em_join[c].fillna(0.0)\n",
    "\n",
    "\n",
    "if comm_meta is not None and len(comm_meta):\n",
    "    person_to_comm = comm_meta[[\"person_id\",\"community_id\"]].drop_duplicates()\n",
    "else:\n",
    "    if comm is None or node_index is None:\n",
    "        raise FileNotFoundError(\"Need Communities_with_meta.parquet OR (Communities.parquet + NodeIndex.parquet).\")\n",
    "    person_to_comm = comm[[\"person_id\",\"community_id\"]].drop_duplicates()\n",
    "\n",
    "em_join = em_join.merge(person_to_comm, on=\"person_id\", how=\"left\")\n",
    "\n",
    "\n",
    "if (risk_threads is not None and len(risk_threads)\n",
    "        and thread_index is not None and len(thread_index)\n",
    "        and \"thread_id\" in thread_index.columns):\n",
    "    th_map = thread_index[[\"email_id\",\"thread_id\"]].drop_duplicates()\n",
    "    rt = risk_threads.rename(columns={\"score\":\"final_score\"})\n",
    "    if \"final_score\" not in rt.columns:\n",
    "        cand = [c for c in rt.columns if \"score\" in c.lower()]\n",
    "        rt[\"final_score\"] = pd.to_numeric(rt[cand[0]], errors=\"coerce\").fillna(0.0) if cand else 0.0\n",
    "    trisk = rt.groupby(\"thread_id\", as_index=False)[\"final_score\"].mean().rename(\n",
    "        columns={\"final_score\":\"thread_risk_mean\"})\n",
    "    em_join = em_join.merge(th_map, on=\"email_id\", how=\"left\").merge(trisk, on=\"thread_id\", how=\"left\")\n",
    "    em_join[\"thread_risk_mean\"] = em_join[\"thread_risk_mean\"].fillna(0.0)\n",
    "\n",
    "per_email = em_join.copy()\n",
    "per_email[\"risk_score\"] = per_email[\"final_score\"].fillna(0.0)\n",
    "\n",
    "grp = per_email.groupby([\"person_id\",\"community_id\"], dropna=False)\n",
    "mean_risk      = grp[\"risk_score\"].mean().rename(\"mean_risk\")\n",
    "mean_sentiment = grp[\"sent_value\"].mean().rename(\"mean_sentiment\") if \"sent_value\" in per_email.columns else None\n",
    "n_emails       = grp[\"email_id\"].nunique().rename(\"n_emails\")\n",
    "\n",
    "\n",
    "if \"top_risk_cat\" in per_email.columns:\n",
    "    cat_scores = (per_email.dropna(subset=[\"top_risk_cat\"])\n",
    "                           .groupby([\"person_id\",\"community_id\",\"top_risk_cat\"])[\"risk_score\"]\n",
    "                           .sum().reset_index())\n",
    "    idx = (cat_scores.sort_values([\"person_id\",\"community_id\",\"risk_score\"], ascending=[True,True,False])\n",
    "                    .drop_duplicates([\"person_id\",\"community_id\"]))\n",
    "    top_cat = idx.set_index([\"person_id\",\"community_id\"])[\"top_risk_cat\"]\n",
    "else:\n",
    "    top_cat = pd.Series(dtype=\"object\")\n",
    "\n",
    "summary_idx = mean_risk.index\n",
    "summary = pd.DataFrame(index=summary_idx).reset_index()\n",
    "summary = summary.merge(mean_risk.reset_index(), on=[\"person_id\",\"community_id\"])\n",
    "if mean_sentiment is not None:\n",
    "    summary = summary.merge(mean_sentiment.reset_index(), on=[\"person_id\",\"community_id\"], how=\"left\")\n",
    "summary = summary.merge(n_emails.reset_index(), on=[\"person_id\",\"community_id\"])\n",
    "summary[\"top_risk_cat\"] = summary.set_index([\"person_id\",\"community_id\"]).index.map(top_cat).values\n",
    "\n",
    "\n",
    "cgrp = summary.groupby(\"community_id\", dropna=False)\n",
    "comm_summary = pd.DataFrame({\n",
    "    \"community_id\": cgrp.size().index,\n",
    "    \"n_people\": cgrp[\"person_id\"].nunique().values,\n",
    "    \"mean_risk\": cgrp[\"mean_risk\"].mean().values,\n",
    "    \"mean_sentiment\": (cgrp[\"mean_sentiment\"].mean().values if \"mean_sentiment\" in summary.columns else np.nan),\n",
    "    \"n_emails\": cgrp[\"n_emails\"].sum().values\n",
    "})\n",
    "\n",
    "if \"top_risk_cat\" in summary.columns:\n",
    "    cat_c = (summary.dropna(subset=[\"top_risk_cat\"])\n",
    "                    .groupby([\"community_id\",\"top_risk_cat\"])\n",
    "                    .agg(weight=(\"mean_risk\",\"mean\"))\n",
    "                    .reset_index())\n",
    "    idx2 = (cat_c.sort_values([\"community_id\",\"weight\"], ascending=[True,False])\n",
    "                  .drop_duplicates([\"community_id\"]))\n",
    "    comm_summary = comm_summary.merge(\n",
    "        idx2.rename(columns={\"top_risk_cat\":\"top_risk_cat_comm\",\"weight\":\"top_cat_weight\"}),\n",
    "        on=\"community_id\", how=\"left\"\n",
    "    )\n",
    "\n",
    "\n",
    "if \"yyyymm\" in per_email.columns:\n",
    "    trend_person = (per_email.groupby([\"yyyymm\",\"person_id\"], dropna=False)\n",
    "                    .agg(mean_risk=(\"risk_score\",\"mean\"),\n",
    "                         mean_sentiment=(\"sent_value\",\"mean\"))\n",
    "                    .reset_index())\n",
    "    trend_comm   = (per_email.groupby([\"yyyymm\",\"community_id\"], dropna=False)\n",
    "                    .agg(mean_risk=(\"risk_score\",\"mean\"),\n",
    "                         mean_sentiment=(\"sent_value\",\"mean\"))\n",
    "                    .reset_index())\n",
    "else:\n",
    "    trend_person = pd.DataFrame(columns=[\"yyyymm\",\"person_id\",\"mean_risk\",\"mean_sentiment\"])\n",
    "    trend_comm   = pd.DataFrame(columns=[\"yyyymm\",\"community_id\",\"mean_risk\",\"mean_sentiment\"])\n",
    "\n",
    "trend_person[\"level\"] = \"person\";  trend_person = trend_person.rename(columns={\"person_id\":\"id\"})\n",
    "trend_comm[\"level\"]   = \"community\"; trend_comm = trend_comm.rename(columns={\"community_id\":\"id\"})\n",
    "trends = pd.concat([trend_person, trend_comm], ignore_index=True)\n",
    "\n",
    "\n",
    "deliver = summary.copy()\n",
    "for col in [\"mean_sentiment\"]:\n",
    "    if col not in deliver.columns: deliver[col] = np.nan\n",
    "deliver = deliver[[\"person_id\",\"community_id\",\"mean_risk\",\"mean_sentiment\",\"top_risk_cat\",\"n_emails\"]]\n",
    "\n",
    "deliver.to_parquet(OUT_DIR / \"RiskSentimentSummary.parquet\", index=False)\n",
    "summary.to_parquet(OUT_DIR / \"PersonSummary.parquet\", index=False)\n",
    "comm_summary.to_parquet(OUT_DIR / \"CommunitySummary.parquet\", index=False)\n",
    "\n",
    "for df in (trend_person, trend_comm):\n",
    "    df[\"yyyymm\"] = df[\"yyyymm\"].astype(\"string\")\n",
    "   \n",
    "    df[\"id\"] = df[\"id\"].astype(\"string\")        # <- key fix\n",
    "    df[\"mean_risk\"] = pd.to_numeric(df[\"mean_risk\"], errors=\"coerce\").astype(\"float32\")\n",
    "    if \"mean_sentiment\" in df.columns:\n",
    "        df[\"mean_sentiment\"] = pd.to_numeric(df[\"mean_sentiment\"], errors=\"coerce\").astype(\"float32\")\n",
    "    else:\n",
    "        df[\"mean_sentiment\"] = np.nan\n",
    "\n",
    "\n",
    "trends = pd.concat([trend_person, trend_comm], ignore_index=True)\n",
    "\n",
    "\n",
    "trends[\"yyyymm\"] = trends[\"yyyymm\"].fillna(\"\")\n",
    "trends[\"id\"] = trends[\"id\"].fillna(\"\")\n",
    "\n",
    "\n",
    "trends.to_parquet(OUT_DIR / \"Trends_monthly.parquet\", index=False)\n",
    "\n",
    "top_people = (summary.sort_values(\"mean_risk\", ascending=False)\n",
    "              .head(10)[[\"person_id\",\"community_id\",\"mean_risk\",\"mean_sentiment\",\"top_risk_cat\",\"n_emails\"]])\n",
    "top_comms  = (comm_summary.sort_values(\"mean_risk\", ascending=False)\n",
    "              .head(10)[[\"community_id\",\"mean_risk\",\"mean_sentiment\",\"n_people\",\"n_emails\",\"top_risk_cat_comm\"]])\n",
    "top_people.to_csv(OUT_DIR / \"Top10_HighRisk_People.csv\", index=False)\n",
    "top_comms.to_csv(OUT_DIR / \"Top10_HighRisk_Communities.csv\", index=False)\n",
    "\n",
    "print(\"✅ Wrote to:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae12dba1-7050-4074-9c39-5bc6c7d41771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
