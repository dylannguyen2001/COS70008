{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64fa64a-9677-44f0-a62f-ac33e6c4fcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Wrote: C:\\Users\\Dylan\\OneDrive - Swinburne University\\COS70008\\Scripts\\Edges.parquet C:\\Users\\Dylan\\OneDrive - Swinburne University\\COS70008\\Scripts\\NodeIndex.parquet C:\\Users\\Dylan\\OneDrive - Swinburne University\\COS70008\\Scripts\\NodeMetrics.parquet\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "INPUT_CLEAN = Path.home() / \"Downloads\" / \"Emails_clean.parquet\"   # where your clean input is\n",
    "OUT_DIR     = Path(r\"C:\\Users\\Dylan\\OneDrive - Swinburne University\\COS70008\\Scripts\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "emails = pd.read_parquet(INPUT_CLEAN, engine=\"pyarrow\", memory_map=False)\n",
    "\n",
    "import re, ast, numpy as np\n",
    "\n",
    "def to_list_clean(x):\n",
    "    \"\"\"Return a clean Python list[str] for any weird cell value.\"\"\"\n",
    "    if x is None:\n",
    "        return []\n",
    "    if hasattr(x, \"as_py\"):\n",
    "        x = x.as_py()\n",
    "    if hasattr(x, \"to_pylist\"):\n",
    "        x = x.to_pylist()\n",
    "\n",
    "    if isinstance(x, (list, tuple, set)):\n",
    "        lst = list(x)\n",
    "    elif isinstance(x, np.ndarray):\n",
    "        lst = x.tolist()\n",
    "    elif isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        if (s.startswith(\"[\") and s.endswith(\"]\")) or (s.startswith(\"(\") and s.endswith(\")\")):\n",
    "            try:\n",
    "                v = ast.literal_eval(s)\n",
    "                if isinstance(v, (list, tuple, set, np.ndarray)):\n",
    "                    lst = list(v)\n",
    "                else:\n",
    "                    lst = [str(v)]\n",
    "            except Exception:\n",
    "                lst = [p.strip() for p in re.split(r\"[;,]\", s) if p.strip()]\n",
    "        else:\n",
    "            lst = [p.strip() for p in re.split(r\"[;,]\", s) if p.strip()]\n",
    "    else:\n",
    "        try:\n",
    "            if pd.isna(x):\n",
    "                return []\n",
    "        except Exception:\n",
    "            pass\n",
    "        lst = [str(x).strip()]\n",
    "\n",
    "    return [str(v).strip().lower() for v in lst if str(v).strip()]\n",
    "\n",
    "for col in [\"to_norm\", \"cc_norm\", \"bcc_norm\"]:\n",
    "    if col not in emails.columns:\n",
    "        emails[col] = [[]] * len(emails)\n",
    "    emails[col] = emails[col].apply(to_list_clean)\n",
    "\n",
    "if \"recipient_count\" not in emails.columns:\n",
    "    emails[\"recipient_count\"] = (\n",
    "        emails[\"to_norm\"].str.len()\n",
    "        + emails[\"cc_norm\"].str.len()\n",
    "        + emails[\"bcc_norm\"].str.len()\n",
    "    ).astype(\"int32\")\n",
    "\n",
    "\n",
    "if \"recipient_count\" not in emails.columns:\n",
    "    emails[\"recipient_count\"] = (\n",
    "        emails[\"to_norm\"].str.len()\n",
    "        + emails[\"cc_norm\"].str.len()\n",
    "        + emails[\"bcc_norm\"].str.len()\n",
    "    ).astype(\"int32\")\n",
    "\n",
    "\n",
    "edges = emails[[\"email_id\", \"person_id\", \"dt_utc\", \"recipient_count\"]].copy()\n",
    "edges[\"recipient\"] = emails[\"to_norm\"] + emails[\"cc_norm\"] + emails[\"bcc_norm\"]\n",
    "edges = edges.explode(\"recipient\", ignore_index=True)\n",
    "\n",
    "edges[\"recipient\"] = edges[\"recipient\"].astype(\"string\").str.lower()\n",
    "edges = edges.dropna(subset=[\"recipient\"])\n",
    "edges = edges[edges[\"recipient\"].str.len() > 0]\n",
    "\n",
    "edges = edges[edges[\"person_id\"] != edges[\"recipient\"]]\n",
    "\n",
    "edges = edges.rename(columns={\"person_id\": \"src_person_id\", \"recipient\": \"dst_person_id\"})\n",
    "edges[\"edge_id\"]      = edges[\"email_id\"] + \"|\" + edges[\"src_person_id\"] + \"|\" + edges[\"dst_person_id\"]\n",
    "edges[\"weight_unit\"]  = 1.0\n",
    "edges[\"weight_mass\"]  = 1.0 / (1.0 + edges[\"recipient_count\"].fillna(0).astype(float))\n",
    "\n",
    "edges_out = edges[[\"edge_id\",\"src_person_id\",\"dst_person_id\",\"email_id\",\"dt_utc\"]].copy()\n",
    "edges_out[\"weight\"]   = edges[\"weight_unit\"].astype(\"float32\")\n",
    "edges_out[\"directed\"] = True\n",
    "edges_out.to_parquet(OUT_DIR / \"Edges.parquet\", index=False)\n",
    "\n",
    "edges_dir = (edges.groupby([\"src_person_id\",\"dst_person_id\"], as_index=False)[\"weight_unit\"]\n",
    "                  .sum().rename(columns={\"weight_unit\":\"weight\"}))\n",
    "edges_dir[\"directed\"] = True\n",
    "\n",
    "u = edges[[\"src_person_id\",\"dst_person_id\",\"weight_mass\"]].copy()\n",
    "u[\"a\"] = np.where(u[\"src_person_id\"] < u[\"dst_person_id\"], u[\"src_person_id\"], u[\"dst_person_id\"])\n",
    "u[\"b\"] = np.where(u[\"src_person_id\"] < u[\"dst_person_id\"], u[\"dst_person_id\"], u[\"src_person_id\"])\n",
    "edges_undir = (u.groupby([\"a\",\"b\"], as_index=False)[\"weight_mass\"]\n",
    "                 .sum().rename(columns={\"a\":\"src_person_id\",\"b\":\"dst_person_id\",\"weight_mass\":\"weight\"}))\n",
    "edges_undir[\"directed\"] = False\n",
    "\n",
    "edges_dir.to_parquet(OUT_DIR / \"Edges_directed_agg.parquet\",   index=False)\n",
    "edges_undir.to_parquet(OUT_DIR / \"Edges_undirected_agg.parquet\", index=False)\n",
    "\n",
    "senders = (emails[[\"person_id\",\"from_norm\",\"internal_sender\",\"domain_sender\"]]\n",
    "           .drop_duplicates()\n",
    "           .rename(columns={\"from_norm\":\"email_norm\",\"internal_sender\":\"internal\",\"domain_sender\":\"domain\"}))\n",
    "\n",
    "recips = edges[[\"dst_person_id\"]].drop_duplicates().rename(columns={\"dst_person_id\":\"person_id\"})\n",
    "recips[\"email_norm\"] = recips[\"person_id\"]\n",
    "recips[\"domain\"]     = recips[\"email_norm\"].str.extract(r'@(.+)$')[0].str.lower()\n",
    "recips[\"internal\"]   = recips[\"domain\"].eq(\"enron.com\")\n",
    "\n",
    "node_index = pd.concat([senders[[\"person_id\",\"email_norm\",\"internal\",\"domain\"]],\n",
    "                        recips[[\"person_id\",\"email_norm\",\"internal\",\"domain\"]]],\n",
    "                       ignore_index=True).drop_duplicates(\"person_id\")\n",
    "node_index.to_parquet(OUT_DIR / \"NodeIndex.parquet\", index=False)\n",
    "\n",
    "Gd = nx.DiGraph()\n",
    "Gd.add_nodes_from(node_index[\"person_id\"])\n",
    "for r in edges_dir.itertuples(index=False):\n",
    "    Gd.add_edge(r.src_person_id, r.dst_person_id, weight=float(r.weight))\n",
    "\n",
    "in_deg  = dict(Gd.in_degree())\n",
    "out_deg = dict(Gd.out_degree())\n",
    "deg     = {n: in_deg.get(n,0) + out_deg.get(n,0) for n in Gd.nodes()}\n",
    "pagerank = nx.pagerank(Gd, alpha=0.85, weight=\"weight\") if Gd.number_of_edges() else {n:0.0 for n in Gd.nodes()}\n",
    "\n",
    "Gu = nx.Graph()\n",
    "Gu.add_nodes_from(node_index[\"person_id\"])\n",
    "for r in edges_undir.itertuples(index=False):\n",
    "    Gu.add_edge(r.src_person_id, r.dst_person_id)\n",
    "\n",
    "clust = nx.clustering(Gu) if Gu.number_of_edges() else {n:0.0 for n in Gu.nodes()}\n",
    "try:\n",
    "    core = nx.core_number(Gu) if Gu.number_of_edges() else {n:0 for n in Gu.nodes()}\n",
    "except nx.NetworkXError:\n",
    "    core = {n:0 for n in Gu.nodes()}\n",
    "\n",
    "wdeg = {n: 0.0 for n in node_index[\"person_id\"]}\n",
    "for r in edges_undir.itertuples(index=False):\n",
    "    wdeg[r.src_person_id] += float(r.weight)\n",
    "    wdeg[r.dst_person_id] += float(r.weight)\n",
    "\n",
    "nm = pd.DataFrame({\"person_id\": node_index[\"person_id\"]})\n",
    "nm[\"degree\"]          = nm[\"person_id\"].map(deg).fillna(0).astype(\"int32\")\n",
    "nm[\"in_degree\"]       = nm[\"person_id\"].map(in_deg).fillna(0).astype(\"int32\")\n",
    "nm[\"out_degree\"]      = nm[\"person_id\"].map(out_deg).fillna(0).astype(\"int32\")\n",
    "nm[\"w_degree\"]        = nm[\"person_id\"].map(wdeg).fillna(0).astype(\"float32\")\n",
    "nm[\"pagerank\"]        = nm[\"person_id\"].map(pagerank).fillna(0).astype(\"float32\")\n",
    "nm[\"clustering_coef\"] = nm[\"person_id\"].map(clust).fillna(0).astype(\"float32\")\n",
    "nm[\"kcore\"]           = nm[\"person_id\"].map(core).fillna(0).astype(\"int32\")\n",
    "nm.to_parquet(OUT_DIR / \"NodeMetrics.parquet\", index=False)\n",
    "\n",
    "print(\"Wrote:\", (OUT_DIR / 'Edges.parquet'),\n",
    "                 (OUT_DIR / 'NodeIndex.parquet'),\n",
    "                 (OUT_DIR / 'NodeMetrics.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f304039-4dd3-44aa-96ec-a115af84f252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
